# 경제 시뮬레이션: AI 기반 모방 메커니즘 설계

## 1. 욕구 체계의 추상화 및 다형성 (재정의)

- **생존 욕구 (Survival Need):** 기본적인 생존에 필요한 재화(필수재) 소비를 통해 충족됩니다.
  - **자녀 양육 욕구 (Child Rearing Need):** 생존 욕구에서 파생되며, 사치재 소비를 통해 충족되는 '사치재'적 성격을 가집니다. 이는 사회적 지위와 연결됩니다.
- **안전 욕구 (Safety Need):** 주변 사람들과의 관계, 영향력, 유사성 등을 통해 충족됩니다.
  - **모방 욕구 (Need for Imitation):** 안전 욕구의 하위 욕구로, 타인의 행동(특히 상위 계층의 사치재 소비)을 모방함으로써 사회적 지위 향상을 꾀하고 안정감을 얻으려는 욕구입니다.

## 2. '사회적 지위' 및 '사치재' 정의

- **사회적 지위 (Social Status):**
  - 에이전트(가계)의 `자산(assets)`과 `사치재(luxury goods)` 보유량에 비례하여 결정되는 수치입니다.
  - `social_status = f(assets, luxury_goods_inventory_value)` 형태로 계산될 수 있습니다. (초기 구현에서는 `luxury_goods_inventory_quantity`를 활용)
- **사치재 (Luxury Goods):**
  - `data/goods.json` 파일 내에서 `is_luxury: true` 속성을 가진 재화로 명시됩니다.
  - 생존에 필수적이지 않지만, 사회적 지위 향상에 기여하는 재화로 간주됩니다.

## 3. '모방 욕구'의 정의 및 영향

- **모방 욕구 증가 요인:**
  - 자신의 `social_status`가 다른 에이전트(특히 상위 계층 에이전트)의 평균 `social_status`보다 현저히 낮을 때.
  - `인정 욕구(recognition_need)`가 높을 때 (기존 `needs`에 포함될 수 있음).
  - 자산은 충분하지만, 아직 사치재를 구매하지 않았을 때.
- **모방 욕구의 영향:**
  - `모방 욕구`가 높을수록 에이전트의 `모방 성향(propensity_to_imitate)`이 증가하며, 이는 사치재 구매 결정에 직접적인 영향을 미칩니다.

## 4. 무엇을 모방할 것인가

- **주요 모방 대상:** 상위 사회적 지위 에이전트의 **사치재 소비 패턴**을 모방합니다.
  - 특정 사치재 구매, 보유량 증가 등이 해당됩니다.
- **확장 가능성:** 추후 특정 `재능/기술(Talent/Skills)`에 대한 투자, 특정 직업 선택 등도 모방 대상으로 고려할 수 있습니다.

## 5. 어떻게 모방할 것인가 (의사결정 과정)

- **타겟 에이전트 식별:**
  - `모방 욕구`가 일정 임계치 이상인 에이전트는 현재 시뮬레이션 내에서 가장 높은 `social_status`를 가진 에이전트(또는 상위 N% 내의 무작위 에이전트)를 모방 타겟으로 식별합니다.
- **타겟의 사치재 소비 관찰:**
  - 식별된 타겟 에이전트가 현재 보유하고 있는 사치재 목록과 수량을 관찰합니다.
- **구매 결정:**
  - 자신의 `모방 성향`, 현재 `자산`, 해당 사치재의 `시장 가격`을 고려하여 관찰된 사치재 중 하나를 구매할지 결정합니다.
  - 구매 결정 시, 자신의 현재 사치재 보유량과 타겟의 보유량을 비교하여 구매 수량을 결정할 수 있습니다. (예: 타겟의 50% 수준까지 맞추기)
- **주문 제출:** 결정된 사치재에 대한 구매 주문을 시장에 제출합니다.

## 6. 피드백 루프 설계

- **양의 피드백:**
  - 사치재 구매에 성공하여 `모방 욕구`가 충족되면, `안전 욕구`가 감소하고 에이전트의 `social_status`가 증가하여 해당 행동이 강화됩니다.
  - 자산 증가, 낮은 욕구 상태 유지, 영향력 있는 상대방 판단 모방 성공 또한 양의 피드백으로 작용합니다.
- **음의 피드백:**
  - 모방 시도(사치재 구매 시도)가 실패(예: 자산 부족, 시장에 재고 없음)하면, `모방 욕구`가 해소되지 않거나 오히려 증가하여 다른 모방 시도를 유발할 수 있습니다.
  - 자산 감소, 욕구 상태 증가, 모방 실패 등은 음의 피드백으로 작용합니다.

## 7. AI 모델 적용 및 학습 목표

- **결정값:** 기존의 판단 객체와 동일한 결과를 산출하도록 합니다. (전략 패턴 적용)
- **학습 목표:**
  - 더 많은 자산의 증가.
  - 더 낮은 욕구 상태 유지 (특히 `모방 욕구` 포함).
  - 화폐 수요 욕구 조정을 통해 자산 증가 목표를 달성할 수 있습니다.
- **학습 주기:** 매 틱마다 적용하는 것을 원칙으로 하되, 설정 가능하도록 유연성을 확보합니다.
- **AI 상태 벡터 강화:** AI가 '현명한 소비'와 같은 행동을 학습할 수 있도록, `AIDecisionEngine`의 `_get_agent_state` 메서드에서 '인지된 평균 가격(perceived_avg_prices)'과 같은 시장 정보를 반드시 상태 벡터에 포함시켜야 합니다.

## 8. 데이터 저장 및 활용

- 매 시뮬레이션 시험마다 판단을 위한 입력값, 다음 판단 값, 보상 함수의 값을 저장합니다.
- 다음 게임 시작 시 해당 데이터를 읽어와서 훈련 후 이를 사용하거나, 전 게임에서 훈련된 결과를 피클링(pickling)하여 불러서 사용합니다.

## 9. 구현 전략

- **전략 패턴:** 의사결정 로직의 전체 또는 일부를 전략 패턴으로 구현하여 유연하게 교체 가능하도록 합니다.
- **추이 분석:** AI 모델의 교체 정도를 바꿔가며 시뮬레이션의 추이를 살펴보고 효과를 분석합니다.

## 10. 현재까지의 구현 및 디버깅 진행 상황 (2025년 7월 6일)

- **기획 구체화**: AI 기반 모방 메커니즘에 대한 상세 기획 (`design/경제시뮬레이션_AI_모방_메커니즘_설계.md`)을 수립했습니다.
- **사치재 정의**: `data/goods.json`에 `is_luxury: true` 속성을 추가하여 사치재를 정의했습니다.
- **설정 값 조정**: `config.py`에 모방 메커니즘 관련 상수들을 추가하고, `IMITATION_NEED_THRESHOLD_FOR_ACTION` 값을 `10.0`으로, `INITIAL_HOUSEHOLD_NEEDS_MEAN["imitation_need"]`를 `1.0`으로 변경했습니다.
- **가계 속성 추가**: `simulation/agents.py`의 `Household` 클래스에 `social_status` 속성을 추가하고, `calculate_social_status` 메서드를 구현했습니다. `update_needs` 메서드에서 `imitation_need`가 `recognition_need`에 비례하여 증가하도록 로직을 추가했습니다.
- **초기 사치재 부여**: `main.py`에서 시뮬레이션 시작 시 일부 가계에 초기 사치재(`luxury_food`)를 부여하여 모방 대상이 존재하도록 했습니다.
- **의사결정 로직 수정**: `simulation/decisions.py`의 `HouseholdDecisionEngine`에 모방 행동 로직을 추가했습니다. 이 로직은 가계의 `imitation_need`와 `social_status`를 기반으로 상위 계층의 사치재 소비를 모방하도록 설계되었습니다.
- **로그 시스템 강화**: `utils/logger.py`를 통해 상세한 디버그 로그를 남기도록 했으며, `log_selector.py`를 개선하여 로그를 효율적으로 필터링하고 분석할 수 있도록 했습니다. 특히 `analyze_imitation_logs.py` 스크립트를 통해 틱(tick) 단위로 로그를 분할하여 `utils/temp` 디렉토리에 저장하도록 구현했습니다.
- **디버깅 현황**: 현재까지의 시뮬레이션 결과, `imitation_need`와 `social_status`는 정상적으로 계산되고 있으며, 모방 대상 가계도 올바르게 식별되고 있습니다. 그러나 `Attempting to buy` 로그가 발생하지 않아 실제 모방 구매가 이루어지지 않고 있습니다. 상세 로그 분석을 통해 `current_quantity < target_quantity`, `available_sell_orders`, `household.assets >= purchase_quantity * purchase_price` 조건 중 어느 부분에서 구매가 막히는지 추가적인 디버깅이 필요한 상황입니다.

## 11. 진화된 AI 훈련 전략 (Evolved AI Training Strategy)

### 11.1. 문제 정의: 현재 훈련 방식의 한계

현재 AI는 매 틱(tick)의 즉각적인 보상만을 기반으로 훈련됩니다. 이 방식은 단기적인 최적화(예: 당장의 허기를 채우기 위한 소비)에는 유용하지만, 다음과 같은 명백한 한계를 가집니다.

1.  **장기적 관점 부재:** 현재의 행동이 여러 틱 후에 가져올 더 큰 보상을 예측하고 학습할 수 없습니다. 예를 들어, 지금 당장 돈을 써서 '학습'을 하면 단기적으로는 자산이 감소하여 손해지만, 미래에는 더 높은 임금을 받아 더 큰 이득을 볼 수 있다는 것을 학습하지 못합니다. 이것이 AI가 '역량 강화'와 같은 장기 투자 행동을 보이지 않는 근본적인 원인입니다.
2.  **제한된 행동 학습:** AI는 즉각적인 보상이 확실한 행동(예: 소비)에만 치중하게 되며, 실패 확률이 있거나 보상이 불확실한 새로운 행동(예: 노동 시장 참여, 가격 협상)을 탐험하려는 동기가 부족합니다. 테스트 결과, AI가 노동 시장에 전혀 참여하지 않는 문제가 이를 증명합니다.
3.  **환경적 제약:** 현재 시뮬레이션에는 주식/금융 시장, 가계 생성(자녀 양육) 등의 시스템이 없어, AI가 '창업'이나 '양육'과 같은 복잡한 행동을 학습하고 실행할 환경이 갖춰져 있지 않습니다.

### 11.2. 해결 방안: 2-Track 훈련 시스템 도입

이러한 한계를 극복하고 더 지능적인 AI를 개발하기 위해, 단기적 훈련과 장기적 훈련을 병행하는 **2-Track 훈련 시스템**을 도입합니다.

#### Track 1: 단기 생존 훈련 (Short-Term Survival Training)

- **목표:** 매 틱의 즉각적인 보상을 통해 기본적인 생존 전략(예: 배고프면 음식 구매, 돈이 없으면 노동 시도)을 빠르고 효과적으로 학습합니다.
- **방법:** 기존의 1-tick 단위 훈련 방식을 유지하고 강화합니다.
- **주요 개선점: 강제 탐험 (Forced Exploration)**
  - AI의 행동 탐험 부족 문제를 해결하기 위해, 특정 조건 하에서 특정 행동을 강제적으로 시도하게 만듭니다.
  - **(이번 구현 범위)** 예를 들어, AI 가계가 실업 상태이고 자산이 일정 수준 이하일 경우, 확률적으로 **노동 시장에 '노동 판매' 주문을 제출하도록 강제**합니다. 이를 통해 AI는 '노동 → 임금 획득 → 자산 증가'라는 핵심적인 인과 관계와 보상 경험을 확실하게 학습하게 됩니다.

### 11.2.1. AITrainingManager의 역할

`AITrainingManager`는 시뮬레이션 전반에 걸쳐 AI 에이전트의 학습 경험을 관리하고, 에피소드 기반 훈련을 조율하는 핵심 컴포넌트입니다. **AITrainingManager는 훈련(경험 수집, 보상 계산, 모델 업데이트)을 전담하며, AIDecisionEngine은 훈련된 모델을 사용하여 순수하게 의사결정만 내리도록 역할을 명확히 분리합니다.** 주요 역할은 다음과 같습니다:

- **AIDecisionEngine 인스턴스 관리:** 각기 다른 `value_orientation`을 가진 `AIDecisionEngine` 인스턴스들을 생성하고 관리합니다. 이를 통해 다양한 가치관을 가진 AI 에이전트들이 독립적으로 학습할 수 있도록 지원합니다.
- **경험 데이터 수집 (`collect_experience`):** 각 틱(tick)마다 에이전트의 행동(pre-state), 그 결과(transactions), 그리고 보상(reward)을 포함하는 경험 데이터를 수집하여 내부 버퍼에 누적합니다. 이 데이터는 AI 모델 훈련에 사용됩니다.
- **에피소드 종료 후 훈련 조율 (`end_episode`):** 정의된 에피소드가 종료될 때마다, 누적된 경험 데이터를 바탕으로 해당 `AIDecisionEngine` 인스턴스들을 훈련시킵니다. 이 과정에서 `SGDRegressor`의 `partial_fit`을 사용하여 점진적 학습을 수행하고, 훈련된 모델을 파일로 저장합니다.
- **보상 계산 (`_calculate_reward`):** 에이전트의 자산 변화, 욕구 충족도, 장기 성장 목표(예: 노동 시장 참여 보상, 생존 욕구 미충족 페널티) 등을 종합적으로 고려하여 각 경험에 대한 보상을 계산합니다.

#### Track 2: 장기 성장 훈련 (Long-Term Growth Training)

- **목표:** 여러 틱에 걸친 누적 보상을 평가하여, 학습/역량 강화와 같은 장기적인 관점의 행동을 학습합니다.
- **방법:** '에피소드(Episode)' 단위의 훈련을 도입합니다.
  1.  **에피소드 정의:** 일정 기간(예: 100 틱) 또는 가계의 한 생애(생성부터 사망까지)를 하나의 '에피소드'로 정의합니다.
  2.  **경험 데이터 누적:** 에피소드가 진행되는 동안 모든 상태, 행동, 보상 데이터를 메모리 버퍼에 계속 저장합니다.
  3.  **에피소드 종료 후 훈련:** 에피소드가 종료되면, 에피소드 전체의 **누적 보상**을 기반으로 모델을 훈련합니다.
  4.  **장기 보상 함수 설계:** `누적 보상 = (에피소드 종료 시점 자산) + (종료 시점 스킬 레벨 × 가중치) - (생존 실패 페널티)` 와 같이 장기적인 성과를 종합적으로 평가하는 보상 함수를 설계합니다.

### 11.3. 장기 과제 (Future Work)

2-Track 훈련 시스템이 안정화된 후, 다음과 같은 과제를 순차적으로 진행합니다.

- **지능적 가격 조정:** AI가 거래 실패 시 '시장의 평균 거래 가격', '자신의 이전 주문 성공 여부' 등의 정보를 상태(state)에 추가하여, 능동적으로 가격을 조정하며 거래 성공률을 높이도록 학습시킵니다.
- **금융 시스템 연동:** 주식, 대출 등 금융 시스템을 구현하고, AI가 이를 활용하여 '창업'이나 '투자'와 같은 고급 행동을 학습하도록 합니다.
- **가계 생성:** '자녀 양육' 행동의 결과로 새로운 가계 에이전트가 생성되는 로직을 구현하고, AI가 세대 전승까지 고려하는 초장기적 전략을 학습하도록 합니다.

### 11.4. 이번 구현 범위 (Current Scope)

이번 개발 주기에서는 **Track 1: 단기 생존 훈련**의 개선에 집중합니다. 구체적인 목표는 다음과 같습니다.

> **`simulation/ai_model.py`의 `AIDecisionEngine`을 수정하여, 실업 상태이고 자산이 부족한 가계 AI가 노동 시장에 참여하여 '노동 판매' 주문을 낼 확률을 인위적으로 높이는 '강제 탐험(Forced Exploration)' 메커니즘을 구현한다.**
