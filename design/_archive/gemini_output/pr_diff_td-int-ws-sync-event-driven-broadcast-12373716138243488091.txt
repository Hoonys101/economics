diff --git a/communications/insights/manual.md b/communications/insights/manual.md
index 210526b6..81b6c514 100644
--- a/communications/insights/manual.md
+++ b/communications/insights/manual.md
@@ -1,35 +1,52 @@
-# Architectural Insights
-1. **Global `sys.modules` Patching in Tests is Harmful:** Manually modifying `sys.modules` in a test file (e.g., `sys.modules["websockets"] = Mock()`) affects the entire test process, including subsequent tests that rely on the real module. This leads to confusing failures like `ModuleNotFoundError` for submodules because the Mock object is not a package.
-2. **Conditional Mocking in `conftest.py` is Robust:** Centralizing mocks for optional dependencies in `conftest.py` ensures that tests can run in minimal environments while still allowing integration tests to use real libraries when available. Adding `mock.__path__ = []` is crucial for mimicking packages to support submodule imports.
+# Architectural Insights: Event-Driven WebSocket Broadcast
+
+## Technical Debt & Architectural Decisions
+
+1.  **Event-Driven vs Polling**: Transitioned `SimulationServer` from a 10Hz polling loop to an event-driven architecture using the Observer pattern. This was achieved by adding `subscribe`/`unsubscribe` methods to the `TelemetryExchange` bridge. This reduces CPU usage during idle times and ensures immediate broadcast upon simulation ticks.
+
+2.  **Thread Safety**: The `TelemetryExchange` bridge manages data updates from the Simulation Thread (Thread A) and notifications to the Server Loop (Thread B). We utilized `asyncio.loop.call_soon_threadsafe` to safely schedule broadcast tasks on the server's event loop from the simulation thread.
+
+3.  **Client State Management**: Initially considered monkey-patching the `websocket` object to store `last_sent_tick`, but refactored to use a dedicated `self.client_states` dictionary in `SimulationServer`. This avoids potential issues with `__slots__` optimization in the `websockets` library and provides a cleaner separation of concerns.
+
+4.  **Race Condition Handling**: Implemented logic to handle the race condition where a client connects and receives an initial snapshot concurrently with a tick broadcast. By tracking `last_sent_tick` per client, we ensure clients receive strictly monotonically increasing ticks and no duplicates.
+
+## Test Evidence
+
+### Unit Tests (`tests/unit/modules/system/test_server_bridge.py`)
+
+```
+...
+----------------------------------------------------------------------
+Ran 3 tests in 0.120s
+
+OK
+```
+
+### Integration Tests (`tests/integration/test_server_integration.py`)
 
-# Test Evidence
 ```
-tests/unit/dashboard/test_socket_manager.py::TestSocketManager::test_audit_logs PASSED [ 16%]
-tests/unit/dashboard/test_socket_manager.py::TestSocketManager::test_initialization PASSED [ 33%]
-tests/unit/dashboard/test_socket_manager.py::TestSocketManager::test_send_command PASSED [ 50%]
-tests/unit/dashboard/test_socket_manager.py::TestSocketManager::test_telemetry_handling PASSED [ 66%]
 tests/integration/test_server_integration.py::test_command_injection
 -------------------------------- live log setup --------------------------------
-INFO     SimulationServer:server.py:27 SimulationServer thread started on localhost:38315
-INFO     websockets.server:server.py:341 server listening on 127.0.0.1:38315
-INFO     websockets.server:server.py:341 server listening on [::1]:38315
-INFO     SimulationServer:server.py:35 WebSocket server running...
+INFO     SimulationServer:server.py:28 SimulationServer thread started on localhost:43739
+INFO     websockets.server:server.py:341 server listening on 127.0.0.1:43739
+INFO     websockets.server:server.py:341 server listening on [::1]:43739
+INFO     SimulationServer:server.py:48 WebSocket server running...
 -------------------------------- live log call ---------------------------------
 INFO     websockets.server:server.py:531 connection open
-INFO     SimulationServer:server.py:43 Client connected: ('::1', 59268, 0, 0)
-INFO     SimulationServer:server.py:58 Client disconnected: ('::1', 59268, 0, 0)
-PASSED                                                                   [ 83%]
+INFO     SimulationServer:server.py:58 Client connected: ('::1', 36568, 0, 0)
+INFO     SimulationServer:server.py:80 Client disconnected: ('::1', 36568, 0, 0)
+PASSED                                                                   [ 50%]
 tests/integration/test_server_integration.py::test_telemetry_broadcast
 -------------------------------- live log setup --------------------------------
-INFO     SimulationServer:server.py:27 SimulationServer thread started on localhost:60639
-INFO     websockets.server:server.py:341 server listening on [::1]:60639
-INFO     websockets.server:server.py:341 server listening on 127.0.0.1:60639
-INFO     SimulationServer:server.py:35 WebSocket server running...
+INFO     SimulationServer:server.py:28 SimulationServer thread started on localhost:34493
+INFO     websockets.server:server.py:341 server listening on 127.0.0.1:34493
+INFO     websockets.server:server.py:341 server listening on [::1]:34493
+INFO     SimulationServer:server.py:48 WebSocket server running...
 -------------------------------- live log call ---------------------------------
 INFO     websockets.server:server.py:531 connection open
-INFO     SimulationServer:server.py:43 Client connected: ('::1', 48328, 0, 0)
-INFO     SimulationServer:server.py:58 Client disconnected: ('::1', 48328, 0, 0)
+INFO     SimulationServer:server.py:58 Client connected: ('::1', 35414, 0, 0)
+INFO     SimulationServer:server.py:80 Client disconnected: ('::1', 35414, 0, 0)
 PASSED                                                                   [100%]
 
-============================== 6 passed in 2.96s ===============================
+============================== 2 passed in 2.88s ===============================
 ```
diff --git a/modules/system/server.py b/modules/system/server.py
index b2d790b1..9e34508b 100644
--- a/modules/system/server.py
+++ b/modules/system/server.py
@@ -4,7 +4,7 @@ import json
 import logging
 import websockets
 from dataclasses import asdict, is_dataclass
-from typing import Optional, List
+from typing import Optional, List, Dict
 from uuid import UUID
 from simulation.dtos.commands import GodCommandDTO
 from modules.system.server_bridge import CommandQueue, TelemetryExchange
@@ -18,6 +18,7 @@ class SimulationServer:
         self.command_queue = command_queue
         self.telemetry_exchange = telemetry_exchange
         self.connected_clients = set()
+        self.client_states: Dict[websockets.WebSocketServerProtocol, int] = {}
         self._stop_event = asyncio.Event()
 
     def start(self):
@@ -30,6 +31,18 @@ class SimulationServer:
         asyncio.run(self._serve())
 
     async def _serve(self):
+        loop = asyncio.get_running_loop()
+
+        # Define callback to be triggered by Simulation Engine (Thread A)
+        # It schedules the broadcast task on the Server Loop (Thread B)
+        def on_tick():
+            loop.call_soon_threadsafe(
+                lambda: asyncio.create_task(self._broadcast_to_all())
+            )
+
+        # Register event listener
+        self.telemetry_exchange.subscribe(on_tick)
+
         try:
             async with websockets.serve(self._handler, self.host, self.port):
                 logger.info("WebSocket server running...")
@@ -37,13 +50,21 @@ class SimulationServer:
                 await asyncio.Future() # run forever
         except Exception as e:
             logger.error(f"Failed to start WebSocket server: {e}")
+        finally:
+            self.telemetry_exchange.unsubscribe(on_tick)
 
     async def _handler(self, websocket):
         self.connected_clients.add(websocket)
         logger.info(f"Client connected: {websocket.remote_address}")
 
-        # Start broadcast task for this client
-        broadcast_task = asyncio.create_task(self._broadcast_loop(websocket))
+        # Initialize per-client state to track sent data
+        # Use explicit dictionary to store client state, avoiding attribute injection on websocket object
+        self.client_states[websocket] = -1
+
+        # Send initial snapshot immediately if available
+        current_snapshot = self.telemetry_exchange.get()
+        if current_snapshot:
+            await self._send_snapshot(websocket, current_snapshot)
 
         try:
             async for message in websocket:
@@ -54,7 +75,8 @@ class SimulationServer:
             logger.error(f"WebSocket handler error: {e}")
         finally:
             self.connected_clients.remove(websocket)
-            broadcast_task.cancel()
+            if websocket in self.client_states:
+                del self.client_states[websocket]
             logger.info(f"Client disconnected: {websocket.remote_address}")
 
     async def _process_message(self, message: str):
@@ -78,17 +100,41 @@ class SimulationServer:
         except Exception as e:
             logger.error(f"Invalid command received: {e}")
 
-    async def _broadcast_loop(self, websocket):
-        last_tick = -1
-        while True:
-            snapshot = self.telemetry_exchange.get()
-            if snapshot and getattr(snapshot, 'tick', -1) > last_tick:
-                try:
-                    # Serialize
-                    payload = asdict(snapshot) if is_dataclass(snapshot) else snapshot
-                    await websocket.send(json.dumps(payload, default=str))
-                    last_tick = snapshot.tick
-                except Exception as e:
-                    logger.error(f"Broadcast failed: {e}")
-                    break
-            await asyncio.sleep(0.1) # 10Hz poll
+    async def _broadcast_to_all(self):
+        """Fetches the latest snapshot and broadcasts to all clients."""
+        snapshot = self.telemetry_exchange.get()
+        if not snapshot:
+            return
+
+        # Iterate over a copy of connected clients to avoid runtime errors during iteration
+        # if a client disconnects concurrently.
+        clients = list(self.connected_clients)
+
+        # Use asyncio.gather to broadcast in parallel, or iterate.
+        # Iterating is safer for exception handling per client.
+        for ws in clients:
+            try:
+                await self._send_snapshot(ws, snapshot)
+            except websockets.exceptions.ConnectionClosed:
+                # Disconnection handled in _handler
+                pass
+            except Exception as e:
+                logger.error(f"Failed to send to client: {e}")
+
+    async def _send_snapshot(self, websocket, snapshot):
+        """Sends the snapshot to a specific client if it's new."""
+        # Check against client's last sent tick
+        last_tick = self.client_states.get(websocket, -1)
+        current_tick = getattr(snapshot, 'tick', -1)
+
+        if current_tick > last_tick:
+            try:
+                # Serialize
+                payload = asdict(snapshot) if is_dataclass(snapshot) else snapshot
+                await websocket.send(json.dumps(payload, default=str))
+
+                # Update client state
+                self.client_states[websocket] = current_tick
+            except Exception as e:
+                # Let caller handle or just re-raise
+                raise e
diff --git a/modules/system/server_bridge.py b/modules/system/server_bridge.py
index 9088b2ac..a106b09e 100644
--- a/modules/system/server_bridge.py
+++ b/modules/system/server_bridge.py
@@ -1,5 +1,5 @@
 import queue
-from typing import Any, Optional
+from typing import Any, Optional, List, Callable
 import threading
 
 # Bridge Types
@@ -8,17 +8,43 @@ CommandQueue = queue.Queue
 class TelemetryExchange:
     """
     Thread-safe container for the latest telemetry snapshot.
+    Supports observer pattern for event-driven updates.
     """
     def __init__(self):
         self._data: Any = None
         self._lock = threading.Lock()
+        self._listeners: List[Callable[[], None]] = []
 
     def update(self, data: Any) -> None:
-        """Atomically updates the snapshot."""
+        """Atomically updates the snapshot and notifies listeners."""
         with self._lock:
             self._data = data
 
+        # Notify listeners outside the lock to prevent deadlocks
+        # Make a copy of listeners to be safe during iteration
+        with self._lock:
+            current_listeners = list(self._listeners)
+
+        for listener in current_listeners:
+            try:
+                listener()
+            except Exception:
+                # Suppress listener errors to avoid breaking the update loop
+                pass
+
     def get(self) -> Optional[Any]:
         """Atomically retrieves the latest snapshot."""
         with self._lock:
             return self._data
+
+    def subscribe(self, callback: Callable[[], None]) -> None:
+        """Registers a callback to be invoked on updates."""
+        with self._lock:
+            if callback not in self._listeners:
+                self._listeners.append(callback)
+
+    def unsubscribe(self, callback: Callable[[], None]) -> None:
+        """Unregisters a callback."""
+        with self._lock:
+            if callback in self._listeners:
+                self._listeners.remove(callback)
diff --git a/tests/unit/modules/system/test_server_bridge.py b/tests/unit/modules/system/test_server_bridge.py
new file mode 100644
index 00000000..e3c228e2
--- /dev/null
+++ b/tests/unit/modules/system/test_server_bridge.py
@@ -0,0 +1,64 @@
+import unittest
+import threading
+from modules.system.server_bridge import TelemetryExchange
+
+class TestTelemetryExchange(unittest.TestCase):
+    def test_update_notifies_listener(self):
+        te = TelemetryExchange()
+        notified = False
+
+        def callback():
+            nonlocal notified
+            notified = True
+
+        te.subscribe(callback)
+        te.update("test_data")
+
+        self.assertTrue(notified)
+        self.assertEqual(te.get(), "test_data")
+
+    def test_unsubscribe_stops_notification(self):
+        te = TelemetryExchange()
+        count = 0
+
+        def callback():
+            nonlocal count
+            count += 1
+
+        te.subscribe(callback)
+        te.update("data1")
+        self.assertEqual(count, 1)
+
+        te.unsubscribe(callback)
+        te.update("data2")
+        self.assertEqual(count, 1) # Should not increment
+        self.assertEqual(te.get(), "data2")
+
+    def test_thread_safety(self):
+        te = TelemetryExchange()
+        errors = []
+
+        def listener():
+            # Simulate slow listener
+            import time
+            time.sleep(0.001)
+
+        te.subscribe(listener)
+
+        def updater():
+            for i in range(100):
+                try:
+                    te.update(i)
+                except Exception as e:
+                    errors.append(e)
+
+        threads = [threading.Thread(target=updater) for _ in range(5)]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        self.assertEqual(len(errors), 0, f"Errors occurred: {errors}")
+
+if __name__ == '__main__':
+    unittest.main()
