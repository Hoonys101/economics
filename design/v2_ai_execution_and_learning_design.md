# V2 AI 전략 실행 및 학습 설계 (계층적 Q-러닝)

## 1. 개요

V2 AI 모델은 **계층적 Q-러닝(Hierarchical Q-Learning)**을 통해, '전략'을 결정하는 상위 AI와 '전술'을 결정하는 하위 AI로 역할을 분리한다.

이 설계 문서는 이 **2중 Q-테이블 구조**가 어떻게 작동하며, AI가 결정한 추상적인 '전술'이 어떻게 구체적인 '행동'으로 변환되고, 그 최종 결과가 어떻게 두 계층의 AI를 학습시키는지 그 표준 프로세스를 정의한다.

---

## 2. 전략 실행 모듈 설계 (Strategy Execution Module)

### 2.2. 구현 방안

- 에이전트 생성 시, 위 3가지 특질 중 하나를 무작위 또는 특정 분포에 따라 할당한다.
- AI의 행동 선택 로직(`choose_action`)에서, 여러 `Intention`의 Q-값이 동일하거나 유사할 경우, 자신의 `Personality`에 해당하는 `Intention`을 선택할 확률을 높인다.

### 2.3. 특질과 욕구 성장 가중치 (Personality and Desire Growth Weights)

에이전트의 '특질'은 단순히 특정 Intention을 선호하는 것을 넘어, 시간이 지남에 따라 각 욕구(Needs)가 증가하는 속도에 직접적인 영향을 미친다. 이는 에이전트의 내적 상태를 지속적으로 형성하며, 특질에 맞는 행동을 유도하는 근본적인 동기가 된다.

-   **구현 방안:**
    -   각 에이전트는 생성 시 할당된 `Personality`에 따라 `desire_weights` 딕셔너리를 가진다.
    -   `desire_weights`는 `survival`, `asset`, `social`, `improvement`와 같은 각 욕구에 대한 성장 가중치를 포함한다.
    -   매 틱마다 `_update_needs` 메서드에서 `BASE_DESIRE_GROWTH` 값에 `desire_weights`를 곱하여 각 욕구를 증가시킨다.
    -   욕구는 특정 행동(예: 자산 획득, 소비)을 통해 해소(감소)될 수 있으며, 이는 보상 계산과 직접적으로 연결된다.

-   **예시 `desire_weights` (가중치 합은 1.0이 아님, 상대적 성장률):
    -   **수전노형 (Asset-Focused):** `{'asset': 1.5, 'social': 0.5, 'improvement': 0.5}`
    -   **지위추구형 (Status-Seeking):** `{'asset': 0.5, 'social': 1.5, 'improvement': 0.5}`
    -   **학습형 (Growth-Oriented):** `{'asset': 0.5, 'social': 0.5, 'improvement': 1.5}`
    -   (생존 욕구는 모든 특질에서 기본 성장률 1.0을 가짐)

### 2.2. 설계 제안: `Agent` 클래스 내 `StrategyExecutor` 역할 부여

AI의 결정을 실행하는 책임은 해당 AI를 소유한 **`Agent` (예: `Firm`, `Household`) 클래스**가 직접 맡는다.

- **위치:** 각 `Agent` 클래스 내부에 `execute_tactic(tactic)`와 같은 메서드를 구현한다.
- **역할:** 이 메서드는 AI로부터 '전술' 객체를 입력받아, 자신의 현재 상태와 시장 데이터를 바탕으로 구체적인 파라미터(가격, 수량)를 계산하고, 최종 `Order` 객체를 생성하여 시장에 제출하는 역할을 수행한다.

### 2.3. 데이터 흐름

1.  **`SimulationEngine`:** `firm.get_tactic()`을 호출하여 `FirmAI`로부터 최종 전술(예: `Tactic.INCREASE_PRICE`)을 얻는다.
2.  **`SimulationEngine`:** `firm.execute_tactic(tactic)`을 호출하여 전술 실행을 명령한다.
3.  **`Firm.execute_tactic`:**
    -   `tactic`이 `INCREASE_PRICE`이면, 자신의 이전 가격, 시장 평균 가격 등을 고려하여 새로운 판매 가격(예: `self.price * 1.05`)을 계산한다.
    -   계산된 가격과 수량을 담아 `Order` 객체를 생성하고 `Market`에 제출한다.

---

## 3. 계층적 Q-러닝 설계 (2중 Q-테이블)

### 3.1. 구조

#### 3.1.1. Q-테이블 1: 전략 AI (Strategic AI)

- **역할:** 에이전트의 전반적인 상태를 보고, 이번 턴의 **최상위 목표(`Intention`)**를 결정한다.
- **상태 (State):** 에이전트의 거시적 상태 (예: `(자산 수준, 주요 욕구)`)
- **행동 (Action):** `Intention` Enum (예: `INCREASE_ASSETS`, `SATISFY_SURVIVAL_NEED`)
- **Q-테이블:** `Q_strategy(상태, Intention)`

#### 3.1.2. Q-테이블 2: 전술 AI (Tactical AI)

- **역할:** 상위 AI가 결정한 `Intention`을 달성하기 위한 **구체적인 행동(`Tactic`)**을 결정한다. 각 `Intention` 별로 개별 Q-테이블이 존재한다.
- **상태 (State):** `Intention`과 관련된 에이전트의 세부 상태
- **행동 (Action):** `Tactic` Enum (예: `Intention`이 `INCREASE_ASSETS`일 경우, `PARTICIPATE_LABOR_MARKET` 또는 `INVEST_IN_STOCKS`)
- **Q-테이블:** `Q_tactic_for_INCREASE_ASSETS(상태, Tactic)`, `Q_tactic_for_SATISFY_SURVIVAL_NEED(상태, Tactic)` 등

### 3.2. 의사결정 흐름

1.  에이전트가 자신의 **전체 상태**를 인식한다.
2.  **전략 AI**가 `Q_strategy` 테이블을 사용하여 최적의 **`Intention`**을 선택한다.
3.  선택된 `Intention`에 해당하는 **전술 AI**가 활성화된다.
4.  **전술 AI**가 자신의 `Q_tactic` 테이블을 사용하여 최적의 **`Tactic`**을 선택한다.
5.  선택된 `Tactic`이 `Agent`의 `execute_tactic` 메서드로 전달되어 실행된다.

### 3.3. 학습 및 보상 적용

1.  **통합 보상 계산:** 틱 종료 시, 행동의 최종 결과(자산 변화, 욕구 변화)를 바탕으로 **단 하나의 통합 보상(Unified Reward)** 값이 계산된다.
2.  **보상 역전파 (Credit Assignment):** 이 통합 보상 값은 의사결정에 참여한 **두 계층의 Q-테이블을 모두 업데이트**하는 데 사용된다.
    -   **전술 테이블 업데이트:** `Q_tactic(상태, Tactic)`이 갱신된다. 이를 통해 전술 AI는 특정 `Intention` 하에서 어떤 `Tactic`이 좋은 최종 결과를 낳는지 학습한다.
    -   **전략 테이블 업데이트:** `Q_strategy(상태, Intention)`가 갱신된다. 이를 통해 전략 AI는 어떤 `Intention`이 장기적으로 높은 보상을 가져다주는지를 학습한다.

### 3.4. 기대 효과

- **효율적 학습:** AI가 '전략'과 '전술'이라는 두 가지 문제를 나누어 학습하므로, 더 빠르고 효율적인 학습이 가능하다.
- **복잡한 행동 학습:** "자산 증대(`Intention`)를 위해, 지금은 임금 협상(`Tactic`)을 하는 것이 투자를 하는 것보다 낫다" 와 같은 복잡한 다단계 의사결정을 스스로 학습하고 수행할 수 있다.

---

## 4. 고급 학습 메커니즘 (설계 아이디어)

**[구현 보류] 아래 내용은 향후 AI 고도화를 위한 설계 아이디어이며, 현재 구현 범위에는 포함되지 않음.**

### 4.1. 동적 학습 초점 (Dynamic Learning Focus)

- **개념:** 실패의 '크기'에 따라 학습의 대상을 동적으로 변경한다.
- **동작:**
    - **큰 손실 발생 시:** 근본적인 **전략(`Intention`)**의 실패로 간주하고, **전략 Q-테이블**의 학습률(`alpha`)을 일시적으로 높여 학습을 집중시킨다.
    - **작은 손실 발생 시:** 세부적인 **전술(`Tactic`)**의 실패로 간주하고, **전술 Q-테이블**의 학습률을 높여 실행 방법을 미세 조정한다.

### 4.2. 학습 강도 조절 (Learning Intensity Adjustment)

- **개념:** 실패의 '연속성'에 따라 학습의 '강도'를 조절한다.
- **동작:**
    - **연속 실패 횟수 기록:** 각 AI 엔진은 행동(`Intention` 또는 `Tactic`)별로 연속 실패 횟수를 기록하는 `consecutive_failures` 딕셔너리를 관리한다.
        - **정의:** '연속 실패'란, 특정 행동의 실패 이후, 다른 행동의 성공/실패 여부와 관계없이 해당 행동이 다시 성공하기 전까지의 누적된 실패를 의미한다. 성공 시에만 해당 행동의 카운트가 0으로 리셋된다.
    - **학습률 증폭:** 특정 행동의 연속 실패 횟수가 늘어날수록, 해당 Q-값을 업데이트하는 학습률(`alpha`)을 점진적으로 증폭시켜(예: `alpha * (1 + 0.5 * failure_count)`), 문제가 되는 행동을 더 빠르고 강하게 교정한다.

### 4.3. 주요 전략 실패 이력 (Major Strategic Failure History)

- **개념:** '큰 손실'을 유발한 전략적 실패는 별도의 이력으로 관리하여 메타 학습의 기반으로 삼는다.
- **동작:**
    - '큰 손실'로 인해 **전략 Q-테이블**의 학습이 발생할 경우, 해당 시점의 상태, 선택했던 `Intention`, 그리고 결과(보상)를 최대 5개까지 기록하는 별도의 '실패 스택(Failure Stack)'에 저장한다.
    - 이 이력은 AI가 특정 상황에서 반복적으로 잘못된 상위 목표를 선택하는 경향이 있는지 등을 분석하거나, 더 고차원적인 학습 로직을 개발하는 데 활용될 수 있다.

### 4.4. 임계값 문제 해결 방안 (Threshold Problem Resolution Strategy)

- **배경:** 위에서 제안된 고급 학습 메커니즘들은 '큰 손실', '연속 실패' 등 임의의 임계값에 의존하는 문제를 가지고 있다. 이는 모델의 유연성과 적응력을 저해할 수 있다.
- **해결 원칙:** 향후 해당 기능들을 구현할 시, 아래와 같은 원칙을 적용하여 임계값 문제를 해결한다.
    1.  **통계 기반 동적 임계값:** 고정된 숫자를 사용하는 대신, 에이전트의 과거 경험 데이터를 바탕으로 임계값을 동적으로 설정한다. 예를 들어, '큰 손실'의 기준은 '최근 N턴 동안의 평균 보상에서 2 표준편차 이상 벗어나는 손실'과 같이 통계적으로 정의한다.
    2.  **설정 파일 기반 변수화:** 동적 임계값 계산에 사용되는 파라미터(예: 표준편차 계수, 관찰 기간 N) 등은 코드에 하드코딩하지 않고, `config.py`와 같은 설정 파일로 분리하여 관리한다. 이를 통해 코드 변경 없이 파라미터를 쉽게 조정하고 실험할 수 있도록 한다.

---

## 5. AI 모델 영속화 전략 (AI Model Persistence Strategy)

### 5.1. 개요

수백 개의 AI 에이전트가 각자 Q-테이블을 가지고 학습하는 환경에서, 학습된 모델을 효율적으로 저장하고 로드하는 영속화 전략이 필요하다. '메모리 내 저장 + 주기적 영속화' 방식을 채택하여, 시뮬레이션 중에는 빠른 접근을 보장하고, 시뮬레이션 간에는 학습된 지식을 보존한다.

### 5.2. 동작 방식

1.  **시뮬레이션 중:** 모든 AI 에이전트의 Q-테이블은 메모리 내에 유지된다. AI의 의사결정 및 Q-값 업데이트는 메모리에서 직접 이루어져 최고 속도를 보장한다.
2.  **영속화 시점:**
    *   시뮬레이션 종료 시점.
    *   특정 턴(예: 1000턴마다) 또는 이벤트(예: 시뮬레이션 일시정지) 발생 시의 체크포인트.
3.  **저장 메커니즘:**
    *   **SQLite 데이터베이스 사용:** 모든 에이전트의 Q-테이블 데이터를 SQLite 데이터베이스에 일괄적으로 저장한다. 이는 `pickle` 파일 개별 저장 방식보다 관리 및 분석에 용이하다.
    *   **데이터 직렬화:** Python 객체 형태의 상태(State)와 행동(Action) 튜플은 데이터베이스에 저장하기 위해 문자열 등으로 직렬화(Serialization)하여 저장한다.

### 5.3. 데이터베이스 스키마 (예시)

-   **`agent_q_values` 테이블:**
    -   `agent_id` (TEXT): 에이전트의 고유 ID (예: `household_001`, `firm_A`)
    -   `q_table_type` (TEXT): '전략'(`strategy`) 또는 '전술'(`tactic`)
    -   `state_key` (TEXT): 직렬화된 상태(State) 튜플의 문자열 표현
    -   `action_key` (TEXT): 직렬화된 행동(Action) 튜플의 문자열 표현
    -   `q_value` (REAL): 해당 상태-행동 쌍의 Q-값

-   **`state_map` 테이블 (선택 사항, 가독성 및 중복 제거용):**
    -   `state_key` (TEXT): `agent_q_values`의 `state_key`와 동일
    -   `original_state_tuple` (TEXT): 직렬화되지 않은 원본 상태 튜플의 문자열 표현 (디버깅 및 분석용)

-   **`action_map` 테이블 (선택 사항, 가독성 및 중복 제거용):**
    -   `action_key` (TEXT): `agent_q_values`의 `action_key`와 동일
    -   `original_action_tuple` (TEXT): 직렬화되지 않은 원본 행동 튜플의 문자열 표현 (디버깅 및 분석용)

### 5.4. 로드 메커니즘

-   시뮬레이션 시작 시, 필요한 모든 Q-테이블 데이터를 SQLite 데이터베이스에서 읽어와 각 AI 에이전트의 메모리에 로드한다.
-   데이터베이스에서 읽어온 직렬화된 상태/행동 키를 다시 Python 튜플 객체로 역직렬화(Deserialization)하여 Q-테이블을 재구성한다.

### 5.5. 기대 효과

-   **확장성:** 수백 개의 Q-테이블을 효율적으로 관리하고 영속화할 수 있다.
-   **성능:** 시뮬레이션 중에는 메모리 내에서 초고속으로 동작하며, 영속화는 일괄 처리로 오버헤드를 최소화한다.
-   **분석 용이성:** 데이터베이스를 통해 학습된 AI 모델의 상태를 쉽게 조회하고 분석할 수 있다.
-   **견고성:** 시뮬레이션이 비정상 종료되더라도 학습된 모델을 복구할 수 있다.