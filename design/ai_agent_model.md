# AI 에이전트 모델 설계 (V2)

## 1. V1 AI 모델의 한계와 V2의 목표

### 1.1. V1 모델의 한계

기존 V1 모델의 AI는 모든 에이전트가 단일 AI를 공유하며, 주로 '가격 맞추기'와 같은 저수준의 전술적 판단에 집중했습니다. 하지만 시장 메커니즘 자체가 불안정했기 때문에, AI가 아무리 합리적인 판단을 하려 해도 경제 전체가 붕괴되는 문제가 발생했습니다. 이는 AI가 행동의 결과(보상)를 정확히 인지하고 학습하기 어려운 환경을 조성하여, 의미 있는 전략 학습이 불가능했습니다.

### 1.2. V2 모델의 목표

V2 설계에서는 AI의 역할을 안정된 시장 위에서 장기적 이익을 추구하는 **고수준의 '전략' 결정**으로 격상시킵니다. 각 에이전트가 독립적인 AI를 통해 자신만의 성공 전략을 학습하고, 시장에 다양한 행태를 창발시키는 것을 목표로 합니다.

## 2. 공통 AI 기반 클래스 설계 (`BaseAIEngine`)

모든 AI 에이전트의 공통적인 학습 메커니즘을 추상화하여 `BaseAIEngine`이라는 '조상 객체'를 설계합니다. 이는 코드의 재사용성, 확장성, 유지보수성을 크게 향상시킵니다.

### 2.1. `BaseAIEngine`의 공통 기능 (구현부)

*   **Q-테이블 자료구조:** `(상태, 행동)` 쌍의 기대 보상(Q-value)을 저장하는 딕셔너리.
*   **핵심 Q-러닝 알고리즘:** 보상을 받았을 때 Q-테이블의 점수를 업데이트하는 로직.
    `Q(s, a) ← Q(s, a) + α * (reward + γ * maxQ(s') - Q(s, a))`
*   **행동 선택 메커니즘:** 현재 상태에서 Q-테이블을 보고 최적의 행동을 선택하는 로직 (일정 확률로 새로운 탐험을 하는 '입실론-그리디' 정책 포함).
*   **학습 프로세스 관리:** `(상태 인식 → 행동 선택 → 보상 획득 → Q-테이블 업데이트)`로 이어지는 전체 학습 사이클을 관장.

### 2.2. 자식 클래스가 정의할 부분 (추상부)

`FirmAI`와 `HouseholdAI`는 `BaseAIEngine`을 상속받아, 자신에게 특화된 다음 3가지 부분만 구현합니다.

1.  **`상태 변환 함수`:** 자신의 복잡한 데이터를 Q-테이블이 사용할 단순한 상태로 변환하는 방법.
2.  **`행동 목록 정의`:** 자신이 할 수 있는 행동들의 목록.
3.  **`보상 계산 함수`:** 행동의 결과로 얻어지는 보상을 계산하는 방법.

## 3. 가계(Household) AI 모델 설계

가계의 복잡한 내부 상태(주관적 욕구)와 외부 상태(객관적 자산)를 AI가 학습 가능한 단순한 상태로 변환하는 것이 핵심입니다.

### 3.1. 상태(State) 정의: 다층적 욕구를 단순 상태로 변환

*   **입력 데이터:**
    1.  `assets` (자산)
    2.  `inventory` (재고 상태)
    3.  `needs` (생존, 인정, 성장 등 6가지 욕구 수치)
    4.  `is_employed` (고용 상태)
*   **상태 변환 로직:**
    1.  **자산 수준 (Asset Level):** 전체 가계의 평균 자산 대비 상대적 위치를 계산하여 `[위험, 안정, 부유]` 3단계로 변환합니다.
    2.  **재고 수준 (Inventory Level):** 핵심 필수품(예: 'food')의 재고량을 기준으로 `[결핍, 충분, 과잉]` 3단계로 변환합니다.
    3.  **주요 욕구 (Primary Need):** 6가지 욕구 중 현재 수치가 가장 높은 **단 하나의 욕구**를 해당 가계의 '주요 욕구'로 정의합니다. (예: `생존`, `인정`, `성장` 등)
*   **최종 상태 (State):** AI는 `(자산 수준, 재고 수준, 주요 욕구)`라는 단순화된 튜플(tuple)을 자신의 현재 상태로 인식합니다. (예: `(자산: 안정, 재고: 충분, 주요 욕구: 생존)`) 

### 3.2. 행동(Action) 정의

AI는 **(전술, 적극성)**의 조합으로 구성된 행동을 선택합니다. 이를 통해 AI는 무엇을 할지뿐만 아니라, 얼마나 공격적으로 할지까지 결정하게 됩니다.

*   **전술 (Tactic):** AI가 수행하려는 구체적인 행동의 종류입니다.
    *   예: `BUY_ESSENTIAL_GOODS`, `PARTICIPATE_LABOR_MARKET`, `TAKE_EDUCATION` 등
*   **적극성 (Aggressiveness):** 해당 전술을 얼마나 공격적으로 수행할지를 나타내는 수준입니다.
    *   `[소극적(PASSIVE), 일반적(NORMAL), 적극적(AGGRESSIVE)]` (3단계)

AI의 최종 행동은 `(Tactic.BUY_ESSENTIAL_GOODS, Aggressiveness.AGGRESSIVE)`와 같은 튜플(tuple) 형태로 결정됩니다. 이 결정은 하위 실행 엔진으로 전달되어 구체적인 주문(가격, 수량) 생성에 사용됩니다.

### 3.3. 보상(Reward) 함수

*   **피드백:** `(자산 증가율) + (주요 욕구 감소율 * 가중치)`
*   자산이 늘어나거나, 가장 시급했던 욕구가 효과적으로 해소되면 높은 보상을 받습니다.

## 4. 기업(Firm) AI 모델 설계

기업은 객관적 상태와 시장 피드백을 바탕으로 이윤 극대화를 위한 전략을 학습합니다.

### 4.1. 상태(State) 정의

*   **입력 데이터:**
    1.  `profit_margin` (최근 N틱 평균 이윤률)
    2.  `inventory_ratio` (목표 재고 대비 현재 재고 비율)
    3.  `hiring_success_rate` (최근 N틱 동안의 채용 성공률)
    4.  `sales_performance` (이전 틱의 판매 실적)
*   **상태 변환 로직:** 각 입력 데이터를 `[낮음, 보통, 높음]` 3단계로 변환합니다. `sales_performance`는 `[매우 부진, 부진, 보통, 완판]` 4단계로 변환합니다.
*   **최종 상태 (State):** `(이윤률, 재고율, 채용률, 판매 실적)` 튜플로 상태를 인식합니다. (예: `(이윤률: 높음, 재고율: 과잉, 채용률: 성공, 판매 실적: 부진)`) 

### 4.2. 행동(Action) 정의

AI는 다음과 같은 **경영 전략** 중 하나를 선택합니다.

*   `가격 정책`: `[인하, 유지, 인상]` (3가지)
*   `임금 정책`: `[인하, 유지, 인상]` (3. V2 시장 메커니즘에 따라 '인하'는 제한될 수 있음)
*   `투자 정책`: `[긴축, 유지, R&D 투자, 생산량 투자]` (4가지)

### 4.3. 보상(Reward) 함수

*   **피드백:** `(기업 자산 증가율)`
*   가장 직관적인 지표인 자산(자본)의 장기적 증감률을 보상으로 삼아, 지속 가능한 성장을 학습 목표로 설정합니다.

## 5. 학습 프로세스

각 에이전트 AI는 자신만의 Q-테이블을 가지고, 행동에 대한 피드백(보상)을 통해 Q-테이블의 값을 업데이트하며 학습합니다. 이 과정은 시뮬레이션 틱마다 반복되어, 에이전트 AI는 점차 자신에게 가장 높은 보상을 가져다주는 전략적 행동을 선택하게 됩니다.

### 5.1. 개체군 기반 진화 학습 (Population-Based Evolutionary Learning)

에이전트의 '죽음'에 대한 피드백은 개별 AI의 Q-테이블 업데이트를 넘어, 개체군 수준의 진화 학습으로 반영됩니다.

*   **도태 (Elimination):** 에이전트가 비활성화(사망)되면, 해당 에이전트가 가지고 있던 Q-테이블은 시뮬레이션에서 **제거(도태)**됩니다. 이는 해당 전략이 '실패'했음을 의미하는 가장 강력한 부정적 피드백입니다.
*   **번식/복제 (Reproduction/Cloning):** 시뮬레이션 내 에이전트의 총 개체수를 유지하거나 늘려야 할 때, '성공적으로 생존하고 번성한' 에이전트의 Q-테이블을 **복제하여 새로운 에이전트에게 부여**합니다. 이때, 복제된 Q-테이블에 약간의 '변이(Mutation)'를 주어 새로운 전략을 탐색할 기회를 제공할 수 있습니다.

### 5.2. 가계 AI의 생존 최우선 학습

가계 AI의 보상 함수는 생존을 최우선 목표로 학습하도록 설계됩니다.

*   **죽음에 대한 강력한 처벌:** 가계 에이전트가 비활성화(사망)되는 순간, 해당 AI는 **매우 크고 즉각적인 부정적 보상**을 받습니다. 이는 AI에게 '이 행동은 절대 피해야 한다'는 강력한 학습 신호가 됩니다.
*   **생존 자체에 대한 보상:** `survival_need`가 낮은 수준으로 유지되고 가계가 활성 상태를 유지하는 것 자체에 대해 작은 긍정적 보상을 지속적으로 부여하여, AI가 '생존'을 최우선 목표로 삼도록 유도합니다.

