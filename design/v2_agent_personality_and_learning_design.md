# V2 AI 에이전트 특질 및 학습 방식 설계

## 1. 개요

모든 AI 에이전트가 동일한 방식으로 학습하고 행동하는 것을 넘어, 각자 고유의 '성향'을 가지도록 설계한다. 이를 위해 에이전트의 행동 경향성을 결정하는 **'특질 (Personality)'**과, 경험으로부터 배우는 방식을 결정하는 **'학습 초점 (Learning Focus)'**이라는 두 가지 새로운 개념을 도입한다.

---

## 2. 에이전트 특질 (Personality) 정의

에이전트의 특질은 Q-러닝의 행동 선택 과정에서, 여러 행동의 기대 보상(Q-value)이 동일할 경우 어떤 **상위 목표(`Intention`)**를 우선적으로 선호할지를 결정하는 경향성이다.

### 2.1. 특질의 종류

1.  **수전노형 (Asset-Focused / Miser):**
    -   **최우선 목표:** `INCREASE_ASSETS` (자산 증대)
    -   **행동 경향:** 단기적인 욕구 충족보다는 자산을 축적하고, 소비를 최소화하는 행동을 선호한다.

2.  **지위추구형 (Status-Seeking):**
    -   **최우선 목표:** `SATISFY_SOCIAL_NEED` (사회적 욕구 충족)
    -   **행동 경향:** 자산을 소모하더라도, 다른 에이전트를 모방하여 사치재를 구매하는 등 사회적 지위를 높이는 행동을 선호한다.

3.  **학습형 (Growth-Oriented):**
    -   **최우선 목표:** `IMPROVE_SKILLS` (역량 강화)
    -   **행동 경향:** 당장의 자산 증대나 소비보다는, 교육/훈련 등 자신의 능력을 향상시키는 장기적인 투자 행동을 선호한다.

### 2.2. 구현 방안

- 에이전트 생성 시, 위 3가지 특질 중 하나를 무작위 또는 특정 분포에 따라 할당한다.
- AI의 행동 선택 로직(`choose_action`)에서, 여러 `Intention`의 Q-값이 동일하거나 유사할 경우, 자신의 `Personality`에 해당하는 `Intention`을 선택할 확률을 높인다.

---

## 3. 학습 초점 (Learning Focus) 특질 설계

'학습 초점'은 계층적 Q-러닝에서, 최종 보상 신호를 **전략 AI(상위)와 전술 AI(하위) 중 어느 쪽에 더 비중을 두어 학습시킬지**를 결정하는 파라미터이다.

### 3.1. `learning_focus` 파라미터

- 각 에이전트는 `learning_focus`라는 0.0에서 1.0 사이의 값을 가진다.
- 이 값은 Q-러닝 업데이트 공식의 학습률(alpha, α)을 조절하는 데 사용된다.

### 3.2. 차등 학습률 (Differential Learning Rates) 적용

- **전략 AI의 학습률:** `alpha_strategy = base_alpha * learning_focus`
- **전술 AI의 학습률:** `alpha_tactic = base_alpha * (1.0 - learning_focus)`

하나의 최종 보상(reward)이 주어졌을 때, 각 계층의 Q-테이블은 자신에게 할당된 학습률에 따라 업데이트된다.

`Q_strategy(s, a) ← Q_strategy(s, a) + alpha_strategy * (delta)`
`Q_tactic(s, a) ← Q_tactic(s, a) + alpha_tactic * (delta)`

### 3.3. 학습 초점의 해석

- **`learning_focus`가 1.0에 가까울수록 (전략가 / Philosopher):**
    - `alpha_strategy`는 커지고 `alpha_tactic`은 0에 가까워진다.
    - 경험의 결과를 자신의 **장기적인 목표(`Intention`)가 올바른지**를 되돌아보는 데 주로 사용한다.
    - 목표 달성 방법(전술)은 잘 바꾸지 않지만, 목표 자체에 대한 수정이 빠르다.

- **`learning_focus`가 0.0에 가까울수록 (기술가 / Tactician):**
    - `alpha_strategy`는 0에 가까워지고 `alpha_tactic`은 커진다.
    - 경험의 결과를 **목표 달성을 위한 `Tactic`을 개선**하는 데 주로 사용한다.
    - 목표 자체는 잘 바꾸지 않지만, 그 목표를 달성하는 기술은 빠르게 개선된다.

## 4. 기대 효과

- **에이전트 다양성 증대:** `특질`과 `학습 초점`이라는 두 축의 조합을 통해, "자신의 신념(전략)은 잘 바꾸지 않지만 돈 버는 기술(전술)은 기가 막히게 발전시키는 수전노(Miser-Tactician)" 또는 "돈 버는 방법은 서툴지만, 끊임없이 더 나은 삶의 목표(전략)를 고민하는 학습가(Growth-Strategist)"와 같이 훨씬 더 다채롭고 현실적인 에이전트들을 시뮬레이션에 등장시킬 수 있다.
