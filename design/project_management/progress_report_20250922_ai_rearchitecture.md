# 진행 보고서: AI 아키텍처 재설계 및 문서화 (2025년 9월 22일)

## 1. 개요

이번 세션에서는 AI 모델의 핵심 아키텍처를 V2 Q-러닝 기반으로 재설계하고, 관련 API 및 학습 메커니즘, 영속화 전략에 대한 상세 설계를 진행했습니다. 또한, 기존 설계 문서들을 최신화된 내용으로 정리하여 프로젝트의 명확성을 높였습니다.

## 2. 주요 진행 상황

### 2.1. AI 모델 재설계 및 API 정의

-   **AI 모델 변경:** 기존 `SGDRegressor` 기반의 AI 모델에서 **계층적 Q-러닝(Hierarchical Q-Learning)** 기반의 V2 AI 모델로 전환되었습니다.
-   **2중 Q-테이블 구조:** AI의 의사결정을 '전략(Intention)'과 '전술(Tactic)'로 분리하고, 각각을 별도의 Q-테이블로 학습시키는 구조가 설계되었습니다.
-   **관심사 분리 리팩토링:** `BaseAIEngine` 클래스의 역할이 너무 크다는 피드백에 따라, `QTableManager`, `ActionSelector`, `LearningTracker` 클래스로 기능을 분리하여 모듈의 응집도를 높였습니다.
-   **핵심 AI API 정의:** `simulation/ai/api.py` 파일에 `Intention` 및 `Tactic` Enum과 `BaseAIEngine` 추상 클래스를 정의하여 AI 모듈의 공용 API를 구축했습니다.

### 2.2. AI 의사결정 및 학습 메커니즘 설계

-   **전략 실행 모듈:** AI가 선택한 추상적인 전략(예: 가격 인상)을 실제 시장에 제출할 구체적인 행동(예: 특정 가격의 주문)으로 변환하는 `Agent` 클래스 내 `execute_tactic` 메서드 설계가 제안되었습니다.
-   **통합 보상 함수 적용:** 최종 결과(자산 변화, 욕구 변화)를 바탕으로 계산된 단일 보상 신호가 계층적 Q-러닝의 두 Q-테이블을 모두 업데이트하는 방식으로 학습이 이루어지도록 설계되었습니다.
-   **고급 학습 메커니즘 (설계 아이디어 - 구현 보류):**
    -   **동적 학습 초점:** 실패의 크기(손실 규모)에 따라 전략 또는 전술 Q-테이블의 학습률을 동적으로 조절하는 방안이 논의되었습니다.
    -   **학습 강도 조절:** 특정 행동의 연속 실패 횟수에 따라 학습률을 증폭시켜 학습 강도를 조절하는 방안이 논의되었습니다. (연속 실패 카운트의 정확한 정의 포함)
    -   **주요 전략 실패 이력:** 큰 손실을 유발한 전략적 실패를 별도 이력으로 기록하여 향후 분석에 활용하는 방안이 논의되었습니다.

### 2.3. AI 모델 영속화 전략

-   **메모리 + 주기적 영속화:** 수백 개의 Q-테이블을 효율적으로 관리하기 위해, 시뮬레이션 중에는 메모리에 유지하고, 시뮬레이션 종료 또는 체크포인트에서 SQLite 데이터베이스에 일괄 저장하는 전략이 채택되었습니다.
-   **데이터베이스 스키마:** Q-테이블 데이터를 저장하기 위한 SQLite 데이터베이스 스키마(예: `agent_q_values` 테이블)가 제안되었습니다.

### 2.4. 설계 문서 정리

-   `design` 폴더의 구조를 최신 V2 아키텍처를 중심으로 재정비했습니다.
-   구버전의 V1 및 AI 피라미드 관련 설계 문서들은 `design/legacy_v1_and_pyramid_designs` 폴더로 아카이빙했습니다.
-   `경제시뮬레이션_AI_모방_메커니즘_설계.md` 문서에 포함된 장기 계획 부분은 새로운 `ai_agent_model.md` 문서에 통합하여 보존했습니다.

## 3. 다음 단계

-   AI 모듈의 구현을 계속 진행합니다. `BaseAIEngine`을 상속받는 `HouseholdAI` 및 `FirmAI` 클래스의 구체적인 구현을 시작할 수 있습니다.
-   설계된 영속화 전략에 따라 Q-테이블을 SQLite에 저장하고 로드하는 기능을 구현합니다.
